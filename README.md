# LLM Evaluation & Regression Testing Platform

> A production-grade framework for preventing silent quality regressions in Large Language Model (LLM) systems.

---

## â“ Problem Statement

Large Language Models are **probabilistic systems**.
They do not fail loudly when prompts, models, or configurations change.

Common failure modes include:
- Increased hallucinations after prompt updates
- Broken output formats after model upgrades
- Silent relevance degradation
- Non-deterministic behavioral drift

Traditional unit tests are **insufficient** for LLM-based systems.

This project addresses that gap.

---

## ğŸ¯ What This Project Does

This platform enables **eval-driven development** for LLM systems by providing:

- Prompt and model versioning
- Golden datasets for deterministic testing
- Automated offline evaluation
- Regression detection against quality baselines
- Experiment tracking and auditability
- Human-readable dashboards for decision-making

The goal is **safe iteration and deployment** of LLM systems.

---

## ğŸ§  Design Principles

- **Model-agnostic**: Works with open-source and closed models
- **Offline-first**: No dependency on paid APIs
- **Deterministic evaluation** of probabilistic systems
- **Metrics over vibes**
- **Reproducibility over convenience**

---

## ğŸ§° Tech Stack

- **LLMs**: Open-source models via Ollama
- **Evaluation**: Custom Python evaluators
- **Experiment Tracking**: MLflow
- **Storage**: SQLite / PostgreSQL
- **Dashboard**: Streamlit
- **CI/CD**: GitHub Actions (planned)

---

## ğŸ“‚ Repository Structure

llm-eval-platform/
â”œâ”€â”€ data/ # Golden datasets
â”œâ”€â”€ prompts/ # Versioned prompts
â”œâ”€â”€ models/ # LLM inference wrappers
â”œâ”€â”€ evals/ # Evaluation metrics & regression logic
â”œâ”€â”€ tracking/ # Experiment tracking integrations
â”œâ”€â”€ dashboard/ # Streamlit UI
â”œâ”€â”€ runs/ # Run configs & outputs
â””â”€â”€ scripts/ # CLI entry points


---

## ğŸš« What This Is NOT

- Not a chatbot
- Not a prompt playground
- Not a demo project
- Not model benchmarking for leaderboards

This is **LLM infrastructure**.

---

## ğŸ§ª Project Status

**Phase 0 â€” Foundation**
- [x] Repository scaffold
- [x] Project vision & principles

Next:
- Phase 1: Golden datasets & prompt versioning

---

## ğŸ“Œ Why This Matters

LLM systems increasingly power critical workflows.
Without regression testing, failures are discovered by users.

This project demonstrates how to build **reliable, testable, and production-safe LLM platforms**.

# llm-eval-platform

## ğŸ§ª Project Status

**Phase 0 â€” Foundation**
- [x] Repository scaffold
- [x] Project vision & principles

**Phase 1 â€” Golden Dataset & Prompt Versioning**
- [x] Golden dataset with deterministic test cases
- [x] Prompt versioning with metadata
- [x] Dataset immutability guarantees

**Phase 2 â€” LLM Inference Engine**
- [x] Model-agnostic inference layer
- [x] Open-source LLM integration via Ollama
- [x] Reproducible run configurations

**Phase 3 â€” Evaluation Framework**
- [x] Faithfulness evaluation
- [x] Relevance evaluation
- [x] Structured format validation
- [x] Offline, deterministic scoring

**Phase 4 â€” Experiment Tracking**
- [x] MLflow-based experiment tracking
- [x] Metric aggregation and logging
- [x] Artifact-level auditability

**Phase 5 â€” Regression Detection**
- [x] Baseline vs candidate comparison
- [x] Threshold-based FAIL / WARN / PASS decisions
- [x] Policy-driven regression rules

**Phase 6 â€” Dashboard**
- [x] Streamlit-based evaluation dashboard
- [x] Aggregate and per-sample metric visualization
- [x] Regression policy transparency
